import { IConnection, HttpError } from "@nestia/fetcher";
import { PlainFetcher } from "@nestia/fetcher/lib/PlainFetcher";
import typia, { tags } from "typia";
import { NestiaSimulator } from "@nestia/fetcher/lib/NestiaSimulator";

import { IRedditLikeModerationAction } from "../../../../../structures/IRedditLikeModerationAction";
import { IPageIRedditLikeModerationAction } from "../../../../../structures/IPageIRedditLikeModerationAction";

/**
 * Create a new moderation action to address reported or problematic content.
 *
 * Create a comprehensive moderation action record documenting a moderator or
 * administrator's decision regarding reported content or proactive content
 * management. This operation is the central mechanism for enforcing community
 * rules and platform policies through formal moderation actions.
 *
 * The operation requires specification of the action type (remove, approve,
 * dismiss_report, escalate, restore, lock), the content being acted upon (post
 * or comment), and detailed reasoning for the action. For removal actions, the
 * system requires additional specification of removal scope (community-level,
 * platform-level, or spam removal). The operation associates actions with
 * triggering reports when applicable, creating a complete audit trail linking
 * user reports to moderator responses.
 *
 * When a moderation action is successfully created, the system immediately
 * executes the specified action on the target content. For removal actions,
 * content is hidden from public view within 1 second per performance
 * requirements. For ban-related actions, user permissions are updated
 * immediately. The system automatically sends notifications to affected content
 * authors informing them of the action taken, the reason, and information about
 * the appeal process per requirements section R-REM-015 through R-REM-017.
 *
 * The operation enforces strict authorization rules based on user role and
 * community scope. Moderators can only create moderation actions for
 * communities where they have moderator permissions, while administrators can
 * take actions across all communities. The system validates that the actor
 * (moderator or admin) has appropriate permissions before executing the
 * action.
 *
 * This endpoint integrates deeply with the Content Moderation and Reporting
 * requirements, implementing the core moderation workflow where reports are
 * reviewed and actions are taken. The created moderation action becomes part of
 * the permanent moderation log per requirements section 9, providing
 * accountability and enabling appeals. The action record includes the actor's
 * identity (moderator or admin), timestamps for audit purposes, and detailed
 * reasoning for transparency.
 *
 * For content removal actions specifically, the system follows the removal
 * process defined in requirements section 6, preserving removed content in the
 * database for audit and appeal purposes while hiding it from public view. The
 * removal type field distinguishes between community-level removals (content
 * hidden from community but visible in user profile) and platform-level
 * removals (content completely hidden from all views).
 *
 * Related operations that may be used together include retrieving content
 * reports to review before taking action, and subsequently checking moderation
 * logs to verify action completion.
 *
 * @param props.connection
 * @param props.body Moderation action details including action type, affected
 *   content, removal scope, and detailed reasoning
 * @path /redditLike/admin/moderation/actions
 * @accessor api.functional.redditLike.admin.moderation.actions.create
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export async function create(
  connection: IConnection,
  props: create.Props,
): Promise<create.Response> {
  return true === connection.simulate
    ? create.simulate(connection, props)
    : await PlainFetcher.fetch(
        {
          ...connection,
          headers: {
            ...connection.headers,
            "Content-Type": "application/json",
          },
        },
        {
          ...create.METADATA,
          path: create.path(),
          status: null,
        },
        props.body,
      );
}
export namespace create {
  export type Props = {
    /**
     * Moderation action details including action type, affected content,
     * removal scope, and detailed reasoning
     */
    body: IRedditLikeModerationAction.ICreate;
  };
  export type Body = IRedditLikeModerationAction.ICreate;
  export type Response = IRedditLikeModerationAction;

  export const METADATA = {
    method: "POST",
    path: "/redditLike/admin/moderation/actions",
    request: {
      type: "application/json",
      encrypted: false,
    },
    response: {
      type: "application/json",
      encrypted: false,
    },
  } as const;

  export const path = () => "/redditLike/admin/moderation/actions";
  export const random = (): IRedditLikeModerationAction =>
    typia.random<IRedditLikeModerationAction>();
  export const simulate = (
    connection: IConnection,
    props: create.Props,
  ): Response => {
    const assert = NestiaSimulator.assert({
      method: METADATA.method,
      host: connection.host,
      path: create.path(),
      contentType: "application/json",
    });
    try {
      assert.body(() => typia.assert(props.body));
    } catch (exp) {
      if (!typia.is<HttpError>(exp)) throw exp;
      return {
        success: false,
        status: exp.status,
        headers: exp.headers,
        data: exp.toJSON().message,
      } as any;
    }
    return random();
  };
}

/**
 * Search and retrieve filtered moderation actions with pagination.
 *
 * Retrieve a comprehensive, searchable list of moderation actions taken across
 * the platform or within specific communities. This operation provides powerful
 * filtering and sorting capabilities essential for moderation oversight, policy
 * enforcement review, and accountability tracking. Moderators can review all
 * actions within their assigned communities, while administrators can query
 * moderation activity platform-wide.
 *
 * The search functionality supports multiple filter dimensions including
 * community scope (filter by specific communities or view all), action types
 * (remove, approve, dismiss, escalate, restore, lock), content types (posts vs
 * comments), status (completed, reversed, appealed), and date ranges for
 * temporal analysis. Additional filters allow searching by the moderator or
 * administrator who performed actions, enabling performance reviews and
 * activity auditing.
 *
 * This operation is critical for several moderation workflows. Moderator teams
 * use it to review recent enforcement decisions for consistency, identify
 * patterns in rule violations, and coordinate moderation strategies across team
 * members. Administrators leverage this endpoint to audit community moderation
 * quality, investigate user complaints about moderator actions, and ensure
 * platform-wide policy compliance.
 *
 * The response provides paginated results with configurable page sizes and
 * sorting options. Users can sort by action timestamp (newest or oldest first),
 * action type, or status to organize results for their specific analysis needs.
 * Each result includes summary information about the action, affected content,
 * and actor, with links to detailed action records.
 *
 * Security and privacy considerations ensure that internal moderator notes and
 * sensitive moderation context are only visible to authorized moderators and
 * administrators. The endpoint enforces strict role-based access control,
 * limiting moderators to viewing actions within their assigned communities
 * while granting administrators full platform visibility.
 *
 * Integration with the broader moderation system includes connections to
 * content reports that triggered actions, appeals filed against actions, and
 * audit logs tracking the complete moderation lifecycle. This enables
 * comprehensive analysis of moderation effectiveness and fairness across the
 * platform.
 *
 * @param props.connection
 * @param props.body Search criteria, filtering options, and pagination
 *   parameters for moderation actions
 * @path /redditLike/admin/moderation/actions
 * @accessor api.functional.redditLike.admin.moderation.actions.index
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export async function index(
  connection: IConnection,
  props: index.Props,
): Promise<index.Response> {
  return true === connection.simulate
    ? index.simulate(connection, props)
    : await PlainFetcher.fetch(
        {
          ...connection,
          headers: {
            ...connection.headers,
            "Content-Type": "application/json",
          },
        },
        {
          ...index.METADATA,
          path: index.path(),
          status: null,
        },
        props.body,
      );
}
export namespace index {
  export type Props = {
    /**
     * Search criteria, filtering options, and pagination parameters for
     * moderation actions
     */
    body: IRedditLikeModerationAction.IRequest;
  };
  export type Body = IRedditLikeModerationAction.IRequest;
  export type Response = IPageIRedditLikeModerationAction;

  export const METADATA = {
    method: "PATCH",
    path: "/redditLike/admin/moderation/actions",
    request: {
      type: "application/json",
      encrypted: false,
    },
    response: {
      type: "application/json",
      encrypted: false,
    },
  } as const;

  export const path = () => "/redditLike/admin/moderation/actions";
  export const random = (): IPageIRedditLikeModerationAction =>
    typia.random<IPageIRedditLikeModerationAction>();
  export const simulate = (
    connection: IConnection,
    props: index.Props,
  ): Response => {
    const assert = NestiaSimulator.assert({
      method: METADATA.method,
      host: connection.host,
      path: index.path(),
      contentType: "application/json",
    });
    try {
      assert.body(() => typia.assert(props.body));
    } catch (exp) {
      if (!typia.is<HttpError>(exp)) throw exp;
      return {
        success: false,
        status: exp.status,
        headers: exp.headers,
        data: exp.toJSON().message,
      } as any;
    }
    return random();
  };
}

/**
 * Retrieve detailed information about a specific moderation action.
 *
 * Retrieve comprehensive details about a single moderation action identified by
 * its unique ID. This operation provides access to the complete record of a
 * moderation decision including what action was taken (remove, approve,
 * dismiss, escalate, restore, lock), who performed the action (moderator or
 * administrator), what content was affected (post or comment), the reasoning
 * behind the decision, and the current status of the action.
 *
 * This endpoint is essential for moderation transparency and accountability.
 * Moderators can review their own past actions to maintain consistency in
 * enforcement. Administrators can audit moderation decisions across the
 * platform to ensure policy compliance and fairness. The operation also
 * supports appeal processes by providing the detailed context users need to
 * understand why their content was moderated.
 *
 * The returned data includes references to the triggering report (if action was
 * report-driven), the affected content (post or comment), the community
 * context, and complete reasoning with both public explanation and internal
 * moderator notes. Status tracking shows whether actions remain in effect, have
 * been reversed, or are under appeal.
 *
 * Security considerations require that internal moderator notes are only
 * visible to users with moderator or administrator privileges. Regular members
 * should not see private moderation team communications. The endpoint enforces
 * role-based access control to ensure only authorized users can view sensitive
 * moderation details.
 *
 * This operation integrates with the moderation appeal system, content
 * reporting workflows, and moderation audit logging. When users appeal
 * moderation decisions, this endpoint provides the foundational data for the
 * appeal review process.
 *
 * @param props.connection
 * @param props.actionId Unique identifier of the moderation action to retrieve
 * @path /redditLike/admin/moderation/actions/:actionId
 * @accessor api.functional.redditLike.admin.moderation.actions.at
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export async function at(
  connection: IConnection,
  props: at.Props,
): Promise<at.Response> {
  return true === connection.simulate
    ? at.simulate(connection, props)
    : await PlainFetcher.fetch(
        {
          ...connection,
          headers: {
            ...connection.headers,
            "Content-Type": "application/json",
          },
        },
        {
          ...at.METADATA,
          path: at.path(props),
          status: null,
        },
      );
}
export namespace at {
  export type Props = {
    /** Unique identifier of the moderation action to retrieve */
    actionId: string & tags.Format<"uuid">;
  };
  export type Response = IRedditLikeModerationAction;

  export const METADATA = {
    method: "GET",
    path: "/redditLike/admin/moderation/actions/:actionId",
    request: null,
    response: {
      type: "application/json",
      encrypted: false,
    },
  } as const;

  export const path = (props: Props) =>
    `/redditLike/admin/moderation/actions/${encodeURIComponent(props.actionId ?? "null")}`;
  export const random = (): IRedditLikeModerationAction =>
    typia.random<IRedditLikeModerationAction>();
  export const simulate = (
    connection: IConnection,
    props: at.Props,
  ): Response => {
    const assert = NestiaSimulator.assert({
      method: METADATA.method,
      host: connection.host,
      path: at.path(props),
      contentType: "application/json",
    });
    try {
      assert.param("actionId")(() => typia.assert(props.actionId));
    } catch (exp) {
      if (!typia.is<HttpError>(exp)) throw exp;
      return {
        success: false,
        status: exp.status,
        headers: exp.headers,
        data: exp.toJSON().message,
      } as any;
    }
    return random();
  };
}
