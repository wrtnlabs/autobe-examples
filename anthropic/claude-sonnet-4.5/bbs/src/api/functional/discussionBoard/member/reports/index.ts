import { IConnection, HttpError } from "@nestia/fetcher";
import { PlainFetcher } from "@nestia/fetcher/lib/PlainFetcher";
import typia from "typia";
import { NestiaSimulator } from "@nestia/fetcher/lib/NestiaSimulator";

import { IDiscussionBoardReport } from "../../../../structures/IDiscussionBoardReport";

/**
 * Submit a new content report for moderation review.
 *
 * Create a new content report flagging discussion topics or replies that
 * violate community guidelines. This operation enables authenticated members to
 * report inappropriate content, triggering the moderation workflow for platform
 * content governance.
 *
 * This endpoint is fundamental to the community-driven moderation system,
 * allowing members to identify violations such as personal attacks, hate
 * speech, misinformation, spam, offensive language, off-topic content, threats,
 * doxxing, trolling, and other guideline violations. By reporting content,
 * members actively participate in maintaining civil discourse standards on
 * economic and political topics.
 *
 * The operation creates a new record in the discussion_board_reports table with
 * all necessary information for moderator review including the reporting
 * member, reported content (either topic or reply), violation category,
 * severity level, and optional explanatory context. The system automatically
 * sets the report status to 'pending' and calculates severity level based on
 * the violation category to prioritize the moderation queue appropriately.
 *
 * Security and validation considerations include verifying the reporting user
 * is an authenticated member with sufficient reputation (minimum 25 reputation
 * points required per business rules), ensuring the reported content exists and
 * is accessible, preventing duplicate reports where the same user reports the
 * same content multiple times within 24 hours, and enforcing rate limits of
 * maximum 10 reports per hour and 50 reports per day per user to prevent report
 * abuse.
 *
 * The system validates that either reported_topic_id or reported_reply_id is
 * provided (but not both), as reports target specific content items. The
 * violation category must be one of the predefined categories: personal_attack,
 * hate_speech, misinformation, spam, offensive_language, off_topic, threats,
 * doxxing, trolling, or other. When 'other' is selected, the
 * reporter_explanation field becomes required with minimum 20 characters.
 *
 * Upon successful report creation, the content is added to the moderation queue
 * where moderators can review, assign themselves, and take appropriate action.
 * Reports with critical severity (hate_speech, threats, doxxing) may trigger
 * automatic content hiding pending moderator review to protect the community
 * from severe violations.
 *
 * This operation integrates closely with the moderation system defined in the
 * requirements, supporting the graduated enforcement approach and transparent
 * moderation processes. Notifications are sent to moderators when new reports
 * are submitted, particularly for high-severity violations requiring immediate
 * attention.
 *
 * The endpoint supports the platform's commitment to community-driven quality
 * control while providing professional moderation infrastructure to address
 * violations consistently and fairly. Member participation in reporting helps
 * moderators identify violations efficiently and demonstrates community
 * investment in maintaining discourse standards.
 *
 * @param props.connection
 * @param props.body Content report submission information including violation
 *   category, reported content reference, and optional explanation
 * @path /discussionBoard/member/reports
 * @accessor api.functional.discussionBoard.member.reports.create
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export async function create(
  connection: IConnection,
  props: create.Props,
): Promise<create.Response> {
  return true === connection.simulate
    ? create.simulate(connection, props)
    : await PlainFetcher.fetch(
        {
          ...connection,
          headers: {
            ...connection.headers,
            "Content-Type": "application/json",
          },
        },
        {
          ...create.METADATA,
          path: create.path(),
          status: null,
        },
        props.body,
      );
}
export namespace create {
  export type Props = {
    /**
     * Content report submission information including violation category,
     * reported content reference, and optional explanation
     */
    body: IDiscussionBoardReport.ICreate;
  };
  export type Body = IDiscussionBoardReport.ICreate;
  export type Response = IDiscussionBoardReport;

  export const METADATA = {
    method: "POST",
    path: "/discussionBoard/member/reports",
    request: {
      type: "application/json",
      encrypted: false,
    },
    response: {
      type: "application/json",
      encrypted: false,
    },
  } as const;

  export const path = () => "/discussionBoard/member/reports";
  export const random = (): IDiscussionBoardReport =>
    typia.random<IDiscussionBoardReport>();
  export const simulate = (
    connection: IConnection,
    props: create.Props,
  ): Response => {
    const assert = NestiaSimulator.assert({
      method: METADATA.method,
      host: connection.host,
      path: create.path(),
      contentType: "application/json",
    });
    try {
      assert.body(() => typia.assert(props.body));
    } catch (exp) {
      if (!typia.is<HttpError>(exp)) throw exp;
      return {
        success: false,
        status: exp.status,
        headers: exp.headers,
        data: exp.toJSON().message,
      } as any;
    }
    return random();
  };
}
