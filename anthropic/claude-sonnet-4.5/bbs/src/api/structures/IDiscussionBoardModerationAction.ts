import { tags } from "typia";

/**
 * Complete moderation action record documenting enforcement actions taken by
 * moderators or administrators.
 *
 * This type represents the comprehensive moderation action entity from the
 * discussion_board_moderation_actions table, serving as the central audit trail
 * for all content governance activities. Each moderation action captures who
 * performed the action, who was affected, what action was taken, why it was
 * taken, which content was involved, and whether the decision was later
 * reversed.
 *
 * Moderation actions are immutable audit records that provide accountability
 * for all enforcement decisions on the platform. They support the graduated
 * enforcement approach where violations escalate from warnings to suspensions
 * to bans, and enable the appeals process where users can contest decisions.
 * All moderation actions are logged with complete context including content
 * snapshots, detailed reasoning, and clear attribution to the acting moderator
 * or administrator.
 *
 * The moderation action entity integrates with related moderation tables
 * including warnings (which reference the moderation action that issued them),
 * suspensions (linked to their initiating moderation action), bans (connected
 * to the action that enacted them), and appeals (which contest specific
 * moderation actions). This relational structure provides complete traceability
 * from initial report through enforcement to potential appeal and resolution.
 *
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export type IDiscussionBoardModerationAction = {
  /**
   * Unique identifier for this moderation action record.
   *
   * This primary key uniquely identifies each moderation action in the
   * discussion_board_moderation_actions table. It is used to reference this
   * specific moderation action from related tables including warnings,
   * suspensions, bans, and appeals.
   *
   * The moderation action ID enables tracking the complete chain of
   * enforcement from initial report through moderation decision to potential
   * appeal and resolution.
   */
  id: string & tags.Format<"uuid">;

  /**
   * Reference to the moderator who performed this moderation action.
   *
   * Identifies the moderator from the discussion_board_moderators table who
   * took enforcement action. Nullable because some actions are performed by
   * administrators rather than moderators.
   *
   * This field supports moderator accountability, performance review, and
   * audit trails showing which moderator made which decisions.
   */
  moderator_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Reference to the administrator who performed this moderation action.
   *
   * Identifies the administrator from the discussion_board_administrators
   * table who took enforcement action. Nullable because some actions are
   * performed by moderators rather than administrators.
   *
   * Administrators have broader authority than moderators and can perform all
   * moderation actions including permanent bans and appeal reversals.
   */
  administrator_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Unique identifier of the member who is the target of this moderation
   * action.
   *
   * References the discussion_board_members table identifying which member is
   * affected by the enforcement action. Required for all moderation actions
   * whether content-specific or account-specific.
   *
   * The target member receives notifications about actions taken and can view
   * moderation decisions affecting them through their account history.
   */
  target_member_id: string & tags.Format<"uuid">;

  /**
   * Reference to the content report that triggered this moderation action.
   *
   * Links to the original report from discussion_board_reports table that
   * prompted moderator review. Nullable for proactive moderation actions
   * taken without user reports.
   *
   * This linkage supports moderation transparency and enables reporters to
   * see resolution outcomes.
   */
  related_report_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Reference to the discussion topic that was subject to moderation.
   *
   * Identifies which topic from discussion_board_topics table was moderated.
   * Nullable for actions not targeting specific topic content.
   *
   * This field enables reviewing moderation decisions in the context of the
   * specific content and supports appeal investigations.
   */
  content_topic_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Reference to the discussion reply that was subject to moderation.
   *
   * Identifies which reply from discussion_board_replies table was moderated.
   * Nullable for actions not targeting specific reply content.
   *
   * Important for maintaining context in threaded discussions and supporting
   * appeals about specific content violations.
   */
  content_reply_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Type of moderation action performed.
   *
   * Specifies the enforcement action taken by the moderator or administrator:
   * hide_content (make invisible to regular users), delete_content
   * (permanently remove from public view), issue_warning (formally warn
   * user), suspend_user (temporarily restrict privileges), ban_user
   * (permanently prevent access), restore_content (reverse previous removal),
   * dismiss_report (determine no violation exists).
   *
   * Each action type triggers specific downstream effects including
   * notifications, privilege changes, and content visibility modifications.
   */
  action_type:
    | "hide_content"
    | "delete_content"
    | "issue_warning"
    | "suspend_user"
    | "ban_user"
    | "restore_content"
    | "dismiss_report";

  /**
   * Detailed explanation of why this moderation action was taken.
   *
   * Provides comprehensive justification for the moderation decision with
   * minimum 20 characters required for substantive explanations. Critical for
   * transparency, accountability, and appeals processing.
   *
   * Moderators explain what guideline was violated, how the content/behavior
   * violated it, and why the specific enforcement action was appropriate.
   * Shown to affected users and reviewed during appeals.
   */
  reason: string;

  /**
   * Category of guideline violation that prompted this action.
   *
   * Categorizes the violation type using standard categories:
   * personal_attack, hate_speech, misinformation, spam, offensive_language,
   * off_topic, threats, doxxing, trolling, or other. Nullable for actions not
   * involving guideline violations.
   *
   * Categorization supports consistent enforcement, pattern analysis, and
   * identification of systemic issues.
   */
  violation_category?:
    | "personal_attack"
    | "hate_speech"
    | "misinformation"
    | "spam"
    | "offensive_language"
    | "off_topic"
    | "threats"
    | "doxxing"
    | "trolling"
    | "other"
    | null
    | undefined;

  /**
   * Preserved snapshot of content as it appeared when action was taken.
   *
   * Captures complete content (topic body or reply content) at moderation
   * moment, creating immutable record for appeals and audit. Nullable for
   * actions not involving specific content.
   *
   * Ensures fair evaluation based on actual content state, prevents
   * retroactive manipulation, and maintains audit integrity for legal
   * compliance.
   */
  content_snapshot?: string | null | undefined;

  /**
   * Whether this moderation action was reversed on appeal or review.
   *
   * Indicates if the moderation decision was overturned after appeal or
   * administrative review. False for original actions, true if subsequently
   * reversed.
   *
   * Reversals occur when appeals demonstrate the original decision was
   * incorrect, providing transparency about moderation accuracy and
   * supporting continuous improvement of moderation practices.
   */
  is_reversed: boolean;

  /**
   * Timestamp when the moderation action was reversed.
   *
   * Records the exact moment when the action was overturned through appeal or
   * administrative review. Nullable if the action has not been reversed.
   *
   * This timestamp tracks the timeline of moderation decisions and their
   * corrections, supporting audit trails and appeal processing workflows.
   */
  reversed_at?: (string & tags.Format<"date-time">) | null | undefined;

  /**
   * Explanation for why the moderation action was reversed.
   *
   * Provides detailed justification when a moderation decision is overturned,
   * explaining what was incorrect about the original decision or what new
   * information prompted the reversal. Nullable if action not reversed.
   *
   * Reversal reasons support learning from moderation mistakes and
   * maintaining transparency about decision corrections.
   */
  reversal_reason?: string | null | undefined;

  /**
   * Timestamp when the moderation action was performed.
   *
   * Records the exact moment when the enforcement action was taken. This
   * immutable timestamp is critical for audit trails, appeal window
   * calculations, and understanding the timeline of moderation events.
   *
   * Creation timestamps enable sorting moderation actions chronologically and
   * measuring moderation response times.
   */
  created_at: string & tags.Format<"date-time">;

  /**
   * Timestamp when the moderation action record was last updated.
   *
   * Tracks when modifications were made to the moderation action record, such
   * as when reversals are applied or additional context is added. Updated
   * automatically when any field changes.
   *
   * This timestamp supports audit trails and helps administrators track the
   * evolution of moderation decisions over time.
   */
  updated_at: string & tags.Format<"date-time">;
};
export namespace IDiscussionBoardModerationAction {
  /**
   * Request body for creating a new moderation action record that documents
   * enforcement actions taken by moderators or administrators.
   *
   * This type defines the complete set of information required when
   * moderators or administrators take enforcement actions including hiding
   * content, deleting content, issuing warnings, suspending users, banning
   * users, restoring previously moderated content, or dismissing user
   * reports. The moderation action serves as the foundational audit record
   * for all content governance activities.
   *
   * Each moderation action must identify who performed the action
   * (moderator_id or administrator_id), which member is affected
   * (target_member_id), what action was taken (action_type), and provide
   * detailed justification (reason with minimum 20 characters). Optional
   * fields link the action to related reports, specific content items,
   * violation categories, and preserve content snapshots for appeals.
   *
   * The moderation action creation integrates with the broader moderation
   * workflow where reports are reviewed, decisions are made, enforcement is
   * applied, and notifications are sent to affected users. All moderation
   * actions are permanently logged for accountability, transparency
   * reporting, and appeals processing.
   */
  export type ICreate = {
    /**
     * Reference to the moderator who performed this moderation action.
     *
     * This field identifies the moderator from the
     * discussion_board_moderators table who took enforcement action against
     * content or a user account. It is nullable because some moderation
     * actions may be performed by administrators rather than moderators.
     *
     * When a moderator creates a moderation action, their moderator ID is
     * recorded here for accountability and audit trail purposes. This
     * enables tracking which moderator made which decisions, supporting
     * moderator performance review and ensuring consistent enforcement of
     * community guidelines.
     *
     * At least one of moderator_id or administrator_id must be provided to
     * identify who performed the action.
     */
    moderator_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Reference to the administrator who performed this moderation action.
     *
     * This field identifies the administrator from the
     * discussion_board_administrators table who took enforcement action
     * against content or a user account. It is nullable because some
     * moderation actions may be performed by moderators rather than
     * administrators.
     *
     * When an administrator creates a moderation action, their
     * administrator ID is recorded here for accountability and audit trail
     * purposes. Administrators have broader authority than moderators and
     * can perform all moderation actions including permanent bans.
     *
     * At least one of moderator_id or administrator_id must be provided to
     * identify who performed the action.
     */
    administrator_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Unique identifier of the member who is the target of this moderation
     * action.
     *
     * This required field references the discussion_board_members table and
     * identifies which member is affected by the moderation enforcement.
     * Every moderation action must target a specific member, whether the
     * action is content-specific (hiding a topic, removing a reply) or
     * account-specific (issuing a warning, suspending access, or banning
     * the account).
     *
     * The target member receives notifications about the moderation action
     * taken against them, including the reason and any available recourse
     * through the appeals process. This field is essential for linking
     * moderation actions to user accounts and maintaining complete
     * moderation history for each member.
     */
    target_member_id: string & tags.Format<"uuid">;

    /**
     * Reference to the content report that triggered this moderation
     * action.
     *
     * This optional field links the moderation action to the original
     * report from the discussion_board_reports table that prompted the
     * moderator review. It is nullable because some moderation actions may
     * be taken proactively by moderators without a user-submitted report.
     *
     * When a moderation action results from a user report, this field
     * establishes the connection between the report and the enforcement
     * action taken. This linkage supports moderation transparency and
     * enables users who submitted reports to see how their reports were
     * resolved.
     */
    related_report_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Reference to the discussion topic that was subject to moderation
     * action.
     *
     * This optional field identifies which topic from the
     * discussion_board_topics table was moderated. It is nullable because
     * not all moderation actions are content-specific (some target user
     * accounts rather than specific content).
     *
     * When a moderation action involves a specific discussion topic (hiding
     * it, removing it, or featuring it), this field captures which topic
     * was affected. This enables administrators to review moderation
     * decisions in the context of the specific content and supports appeal
     * investigations.
     */
    content_topic_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Reference to the discussion reply that was subject to moderation
     * action.
     *
     * This optional field identifies which reply from the
     * discussion_board_replies table was moderated. It is nullable because
     * not all moderation actions are content-specific (some target user
     * accounts or discussion topics rather than individual replies).
     *
     * When a moderation action involves a specific reply (hiding it,
     * removing it, or flagging it), this field captures which reply was
     * affected. This is important for maintaining context in threaded
     * discussions and supporting appeals where users need to understand
     * which specific content violated guidelines.
     */
    content_reply_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Type of moderation action being performed.
     *
     * This required field specifies what enforcement action the moderator
     * or administrator is taking. The action types form the vocabulary of
     * available moderation interventions, ranging from content management
     * to user account restrictions.
     *
     * Available action types include: hide_content (make content invisible
     * to regular users while preserving for audit), delete_content
     * (permanently remove content from public view), issue_warning
     * (formally warn user about guideline violation), suspend_user
     * (temporarily restrict posting privileges), ban_user (permanently
     * prevent platform access), restore_content (reverse previous content
     * removal), and dismiss_report (determine reported content does not
     * violate guidelines).
     *
     * Each action type triggers different downstream effects including
     * notification generation, user privilege changes, and content
     * visibility modifications.
     */
    action_type:
      | "hide_content"
      | "delete_content"
      | "issue_warning"
      | "suspend_user"
      | "ban_user"
      | "restore_content"
      | "dismiss_report";

    /**
     * Detailed explanation of why this moderation action was taken.
     *
     * This required field provides comprehensive justification for the
     * moderation decision, with a minimum length of 20 characters to ensure
     * substantive explanations. The reason is critical for moderation
     * transparency, accountability, and supporting the appeals process.
     *
     * Moderators must clearly explain what guideline was violated, how the
     * content or behavior violated that guideline, and why the specific
     * enforcement action was appropriate given the violation severity. The
     * reason is shown to the affected user in moderation notifications and
     * is reviewed by administrators during appeals processing.
     *
     * Detailed reasoning supports consistent moderation practices, helps
     * users understand community standards, and enables fair evaluation of
     * moderation decisions during appeals.
     */
    reason: string;

    /**
     * Category of guideline violation that prompted this moderation action.
     *
     * This optional field categorizes the type of community guideline
     * violation, using the same violation categories as content reports. It
     * is nullable for actions that do not involve guideline violations
     * (such as restoring content or dismissing reports).
     *
     * Violation categories include: personal_attack (direct insults or
     * harassment), hate_speech (promoting hatred against groups),
     * misinformation (deliberate false information), spam (repetitive or
     * commercial content), offensive_language (profanity or vulgar
     * expressions), off_topic (content unrelated to economics/politics),
     * threats (explicit or implicit harm), doxxing (sharing private
     * information), trolling (intentionally disruptive behavior), and other
     * (violations not covered by standard categories).
     *
     * Categorizing violations supports consistent enforcement, enables
     * pattern analysis across moderation decisions, and helps identify
     * systemic issues or emerging violation types.
     */
    violation_category?:
      | "personal_attack"
      | "hate_speech"
      | "misinformation"
      | "spam"
      | "offensive_language"
      | "off_topic"
      | "threats"
      | "doxxing"
      | "trolling"
      | "other"
      | null
      | undefined;

    /**
     * Preserved snapshot of the content exactly as it appeared when the
     * moderation action was taken.
     *
     * This optional field captures the complete content (topic body or
     * reply content) at the moment of moderation action, creating an
     * immutable record for appeal review and audit purposes. It is nullable
     * for actions that do not involve specific content (such as user
     * suspensions without content reference).
     *
     * The content snapshot is critical because it ensures that even if the
     * user subsequently edits or deletes the content, the moderation
     * decision can be fairly evaluated based on what the content actually
     * said at the time of the violation. This protects both users and
     * moderators during appeals by providing objective evidence of the
     * content state.
     *
     * Preserving content snapshots supports transparent moderation, enables
     * fair appeals processing, prevents retroactive content manipulation,
     * and maintains audit integrity for legal and compliance requirements.
     */
    content_snapshot?: string | null | undefined;
  };

  /**
   * Search and filtering criteria for retrieving moderation queue reports
   * with pagination and sorting.
   *
   * This request type supports the moderation workflow by enabling moderators
   * and administrators to efficiently locate and prioritize content reports
   * requiring review. The filtering capabilities help moderators focus on
   * specific violation types, severity levels, or time periods while the
   * sorting and pagination controls enable efficient queue processing.
   *
   * The flexible parameter combination supports various moderation workflows
   * including reviewing all pending critical violations, monitoring specific
   * moderators' queues, investigating reports from particular timeframes, and
   * identifying content with multiple community reports indicating consensus
   * violations.
   */
  export type IRequest = {
    /**
     * Filter by report status including pending (unassigned reports
     * awaiting moderator review), under_review (reports actively being
     * investigated by assigned moderators), resolved (completed reviews
     * with moderation actions taken), or dismissed (reports determined to
     * be without merit or false flags).
     *
     * This filter helps moderators organize their workflow by viewing only
     * reports in specific processing stages.
     */
    status?: string | undefined;

    /**
     * Filter by violation severity level for prioritizing critical content
     * moderation needs.
     *
     * Values include critical (hate speech, threats, doxxing requiring
     * immediate action), high (personal attacks, targeted harassment
     * needing urgent response), medium (offensive language, misinformation,
     * trolling for standard enforcement), or low (off-topic content, minor
     * violations for educational approach).
     *
     * Severity-based filtering enables moderators to triage the queue
     * efficiently and address the most serious violations first.
     */
    severity?: string | undefined;

    /**
     * Filter by specific violation category to focus moderation efforts on
     * particular guideline violation types.
     *
     * Categories include personal_attack (direct insults targeting users),
     * hate_speech (content promoting hatred against groups), misinformation
     * (deliberate spread of false information), spam (repetitive or
     * commercial content), offensive_language (profanity or vulgar
     * expressions), off_topic (unrelated to economics/politics), threats
     * (explicit or implicit harm threats), doxxing (sharing private
     * information), trolling (intentionally disruptive behavior), or other
     * (violations not covered by standard categories).
     *
     * This enables specialized moderation workflows and helps moderators
     * develop expertise in specific violation types.
     */
    violation_category?: string | undefined;

    /**
     * Filter by the moderator assigned to review reports.
     *
     * This UUID references the discussion_board_moderators table and allows
     * filtering to view reports assigned to specific moderators, unassigned
     * reports (null value), or reports in a particular moderator's queue.
     *
     * Useful for moderator performance tracking, workload balancing, and
     * personal queue management.
     */
    assigned_moderator_id?: (string & tags.Format<"uuid">) | undefined;

    /**
     * Start date for filtering reports by submission timestamp.
     *
     * Returns only reports created on or after this ISO 8601 formatted
     * date-time value. Used in combination with date_to to define specific
     * time windows for report investigation or to focus on recent
     * moderation activity.
     */
    date_from?: (string & tags.Format<"date-time">) | undefined;

    /**
     * End date for filtering reports by submission timestamp.
     *
     * Returns only reports created on or before this ISO 8601 formatted
     * date-time value. Combined with date_from to analyze reports within
     * specific timeframes or historical periods.
     */
    date_to?: (string & tags.Format<"date-time">) | undefined;

    /**
     * Minimum number of reports on the same content item.
     *
     * Filters to show only content that has been reported by multiple users
     * (e.g., report_count_min: 3 shows content flagged by 3 or more
     * different users).
     *
     * Multiple reports indicate community consensus about violations and
     * help prioritize content that multiple users find problematic.
     */
    report_count_min?: (number & tags.Type<"int32">) | undefined;

    /**
     * Page number for pagination of report results.
     *
     * Defaults to 1 if not specified. Used with limit to navigate through
     * large moderation queues efficiently.
     */
    page?: (number & tags.Type<"int32">) | undefined;

    /**
     * Number of report records to return per page.
     *
     * Defaults to 20 items per page. Maximum allowed is typically 100 to
     * prevent excessive data transfer. Controls the size of each paginated
     * result set for moderation queue display.
     */
    limit?: (number & tags.Type<"int32">) | undefined;

    /**
     * Field to sort results by for organizing the moderation queue.
     *
     * Options include priority (default composite score based on severity,
     * multiple reports, and queue time), created_at (chronological
     * submission order), severity (violation severity level), report_count
     * (number of reports on same content), or updated_at (most recently
     * modified reports).
     *
     * Sorting helps moderators work through the queue efficiently with
     * critical items appearing first.
     */
    sort_by?: string | undefined;

    /**
     * Sort direction for organizing report results.
     *
     * Values are asc (ascending, oldest or lowest first) or desc
     * (descending, newest or highest first).
     *
     * Default is desc for priority and severity sorting to show critical
     * items first, and can be reversed for chronological review workflows.
     */
    sort_order?: string | undefined;
  };
}
